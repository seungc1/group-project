# -*- coding: utf-8 -*-
"""talktotext_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eqGxsoxlXXB1Xb9t2MkfomT_mNZTpioG
"""

# 코랩 전용 테스트 코드: Flask 없이 Whisper + Pyannote + 키워드 + 요약 테스트
!pip install -q git+https://github.com/openai/whisper.git
!pip install -q pyannote.audio pydub konlpy ffmpeg-python

import whisper
import os
import requests
from pydub import AudioSegment
from collections import Counter
from konlpy.tag import Okt
from pyannote.audio.pipelines import SpeakerDiarization

# 사용자 입력 필요 (토큰, API 키)
HUGGINGFACE_TOKEN = "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"
UPSTAGE_API_KEY = "up_sH6XQ36Gg4Sgu1iso3fin3jWYtBFL"

# 오디오파일 업로드 (.wav)
from google.colab import files
uploaded = files.upload()
audio_file = list(uploaded.keys())[0]
base_filename = os.path.splitext(audio_file)[0]

# 오디오 분할 1분 단위로
chunk_length_ms = 60 * 1000
os.makedirs("chunks", exist_ok=True)
audio = AudioSegment.from_file(audio_file)
chunk_files = []
for i in range(0, len(audio), chunk_length_ms):
    chunk = audio[i:i+chunk_length_ms]
    chunk_name = f"chunks/chunk_{i//chunk_length_ms + 1}.wav"
    chunk.export(chunk_name, format="wav")
    chunk_files.append(chunk_name)

# Whisper로 변환
model = whisper.load_model("large-v2")
segments_all = []
full_text = ""
for chunk in chunk_files:
    result = model.transcribe(chunk)
    segments_all.extend(result["segments"])
    full_text += result["text"].strip() + "\n"

# 화자 분리
pipeline = SpeakerDiarization.from_pretrained("pyannote/speaker-diarization", use_auth_token=HUGGINGFACE_TOKEN)
diarization = pipeline(audio_file)

# 화자 매핑
def format_timestamp(start, end):
    return f"[{int(start//60):02d}:{int(start%60):02d} → {int(end//60):02d}:{int(end%60):02d}]"

transcript = []
for segment in segments_all:
    mid = (segment["start"] + segment["end"]) / 2
    speaker = "Unknown"
    for turn, _, label in diarization.itertracks(yield_label=True):
        if turn.start <= mid <= turn.end:
            speaker = label
            break
    transcript.append(f"{format_timestamp(segment['start'], segment['end'])} ({speaker}): {segment['text'].strip()}")

# 키워드 추출
okt = Okt()
nouns = okt.nouns(full_text)
keywords = [word for word, _ in Counter([n for n in nouns if len(n) > 1]).most_common(5)]

# 업스테이지 요약
summary = "요약 실패"
try:
    response = requests.post(
        "https://api.upstage.ai/v1/chat/completions",
        headers={"Authorization": f"Bearer {UPSTAGE_API_KEY}", "Content-Type": "application/json"},
        json={
            "model": "solar-pro",
            "messages": [
                {"role": "system", "content":
                 "현재 스마트 회의록 작성을 진행하고 있어. 다음 텍스트를 실제 업무에서 사용하는 회의록 형식으로 작성해줘."},
                {"role": "user", "content": full_text}
            ]
        }
    )
    summary = response.json()["choices"][0]["message"]["content"]
except Exception as e:
    print("요약 실패:", e)

# 출력
print("\n--- 화자별 회의록 ---")
print("\n".join(transcript))
print("\n--- 키워드 ---")
print(keywords)
print("\n--- 요약 ---")
print(summary)

# 저장 및 다운로드
with open(f"{base_filename}_transcript.txt", "w", encoding="utf-8") as tf:
    tf.write("\n".join(transcript))

with open(f"{base_filename}_final.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(transcript))
    f.write("\n\n[키워드]\n" + ", ".join(keywords))
    f.write("\n\n[요약]\n" + summary)

files.download(f"{base_filename}_transcript.txt")
files.download(f"{base_filename}_final.txt")
