# -*- coding: utf-8 -*-
"""talktotext.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FcKzzlb88-z2lMv5n7TjO30MQw5srnX1
"""

# 필요 라이브러리 설치 (whisper, pyannote, ffmpeg)
!pip install -qq git+https://github.com/openai/whisper.git
!pip install -qq pyannote.audio
!pip install -qq ffmpeg-python
!pip install pydub

import whisper
import os
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.core import Segment
from google.colab import files
from pydub import AudioSegment

# Hugging Face Token (화자분리용)
HUGGINGFACE_TOKEN = "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"

# 모델 로드
model = whisper.load_model("medium")

# 오디오 업로드
uploaded = files.upload()
audio_file = list(uploaded.keys())[0]
base_filename = os.path.splitext(audio_file)[0]

# 오디오 로딩 (pydub)
audio = AudioSegment.from_file(audio_file)

# 1분 단위로 분할
chunk_length_ms = 60 * 1000
os.makedirs("chunks", exist_ok=True)
chunk_files = []

for i in range(0, len(audio), chunk_length_ms):
    chunk = audio[i:i + chunk_length_ms]
    chunk_filename = f"chunks/chunk_{i // chunk_length_ms + 1}.wav"
    chunk.export(chunk_filename, format="wav")
    chunk_files.append(chunk_filename)
    print(f"청크 저장: {chunk_filename}")

# Whisper 청크별 처리
full_transcript = ""
for chunk_file in chunk_files:
    print(f"처리 중: {chunk_file}")
    result = model.transcribe(chunk_file)
    full_transcript += result["text"].strip() + "\n\n"

# 전체 결과 저장
with open("final_transcript_from_chunks.txt", "w", encoding="utf-8") as f:
    f.write(full_transcript)

print("전체 회의록 저장 완료!")

# 화자 분리 (전체 오디오 기준)
result = model.transcribe(audio_file, verbose=False)
segments = result["segments"]
pipeline = SpeakerDiarization.from_pretrained("pyannote/speaker-diarization", use_auth_token=HUGGINGFACE_TOKEN)
diarization = pipeline(audio_file)

# 회의록 타임스탬프 + 화자 매핑
def format_timestamp(start, end):
    return f"[{int(start//60):02d}:{int(start%60):02d} → {int(end//60):02d}:{int(end%60):02d}]"

def generate_speaker_transcript(segments, diarization, output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        for segment in segments:
            mid = (segment["start"] + segment["end"]) / 2
            speaker_label = "Unknown"
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                if turn.start <= mid <= turn.end:
                    speaker_label = speaker
                    break
            timestamp = format_timestamp(segment["start"], segment["end"])
            text = segment["text"].strip()
            line = f"{timestamp} ({speaker_label}): {text}"
            print(line)
            f.write(line + "\n")
    print(f"\n 화자 매핑 회의록 저장 완료: {output_file}")

# 실행
final_txt = f"{base_filename}_final_transcript.txt"
generate_speaker_transcript(segments, diarization, final_txt)

# 자동 다운로드
files.download(final_txt)
